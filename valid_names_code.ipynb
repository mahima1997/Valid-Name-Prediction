{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code prepares a model to classify a name as valid or invalid. Our target is to prepare a model which has a high precision in identifying an invalid name as per our demand where we cant take a chance with an invalid name being classified as valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random \n",
    "import string\n",
    "import functools\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import re \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import functools\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import Callback \n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_and_confusion_report(actual_label,predicted_label,threshold):\n",
    "    predicted_label = np.where(predicted_label>threshold,1,0)\n",
    "    report=classification_report(actual_label,predicted_label)\n",
    "    cm = confusion_matrix(actual_label,predicted_label)\n",
    "    print(cm)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a preprocessing function where regex is used to deal with the given valid names in our dataset. \n",
    "All the individually occuring letters are deleted along with the spaces, dots and commas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing names to have reduced variance in training teh n/w\n",
    "def preprocessing(df):\n",
    "    df[\"name\"]=df[\"name\"].str.lower()\n",
    "    df[\"name\"]=df[\"name\"].apply(lambda x: re.sub(r\"\\s[a-z]\\s\", \"\", x))\n",
    "    df[\"name\"]=df[\"name\"].apply(lambda x: re.sub(r\"\\.[a-z]\\.\", \"\", x))\n",
    "    df[\"name\"]=df[\"name\"].apply(lambda x: re.sub(r\"^[a-z]\\.\", \"\", x))\n",
    "    df[\"name\"]=df[\"name\"].apply(lambda x: re.sub(r\"^[a-z]\\s\", \"\", x))\n",
    "    df[\"name\"]=df[\"name\"].apply(lambda x: re.sub(r\"\\s[a-z]$\", \"\", x))\n",
    "    df[\"name\"]=df[\"name\"].apply(lambda x: re.sub(r\"\\d\", \"\", x))   #integer matching\n",
    "    df[\"name\"]=df[\"name\"].apply(lambda x: x.replace(\".\",\"\"))\n",
    "    df[\"name\"]=df[\"name\"].apply(lambda x: x.replace(\",\",\"\"))\n",
    "    df[\"name\"]=df[\"name\"].apply(lambda x: x.strip())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random_name_given_length() is the function to generate random names of a given length from available lower case characters in english dictionary along with a space and dot which are invalid. There is very less probability of having a randomly generated name being valid and hence all of them are tagged as invalid only. It is kept in mind that lengths of invalid names are generated in proportion equal to the one available in our valid_name dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_name_given_length(length):\n",
    "    letters=list(string.ascii_lowercase+'.'+' ')\n",
    "    return ''.join(random.choice(letters) for i in range(length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete data is loaded with valid + invalid names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(times):\n",
    "    \n",
    "    valid_data = pd.read_csv(\"names.csv\")\n",
    "    valid_data[\"is_valid\"] = 1\n",
    "    valid_data.dropna(inplace=True)\n",
    "\n",
    "    #creating some invalid names for training purpose\n",
    "    invalid_data = pd.DataFrame()\n",
    "    for i in range(times):\n",
    "        invalid_data_subset = valid_data[\"name\"].apply(lambda x : random_name_given_length(len(x))).reset_index()\n",
    "        invalid_data_subset[\"is_valid\"] = 0    \n",
    "        invalid_data = pd.concat([invalid_data,invalid_data_subset], ignore_index=True)\n",
    "    \n",
    "    #concatenating datasets\n",
    "    data = pd.concat([valid_data,invalid_data], ignore_index=True)\n",
    "    \n",
    "    #preprocessing\n",
    "    data = preprocessing(data)\n",
    "\n",
    "    #Shuffling data \n",
    "    data = shuffle(data)    \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_keras_metric(method):\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self, args, **kwargs):\n",
    "        \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
    "        value, update_op = method(self, args, **kwargs)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            value = tf.identity(value)\n",
    "        return value\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncating length of names to length of 25 and equating MAX_SEQUENCE_LENGTH=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH=25\n",
    "\n",
    "def char2sequence(df,flag):\n",
    "    \n",
    "    if flag=='train':\n",
    "        tokenizer.fit_on_texts(df.astype(str))\n",
    "    \n",
    "    df_features=tokenizer.texts_to_sequences(df.astype(str))\n",
    "    df_features = pad_sequences(df_features, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    word_index = tokenizer.word_index\n",
    "    return df_features,word_index\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering Gupta Mahima to be a valid name given Mahima Gupta to be valid in order to create more valid names, augmentation is done for the given dataset and only on the training part.\n",
    "Also the complete dataset is divided into train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_array(data,test_fraction,augmentation):\n",
    "    \n",
    "    #import pdb;pdb.set_trace()\n",
    "\n",
    "    # Lets split data here \n",
    "    msk = np.random.rand(len(data)) < test_fraction\n",
    "    \n",
    "    train=data[msk]\n",
    "    test=data[~msk]\n",
    "    \n",
    "    #Augmenting valid data\n",
    "    \n",
    "    if augmentation:        \n",
    "        data_augmented = pd.DataFrame()\n",
    "        #Only training data can be used for augmenting\n",
    "        data_augmented[\"name\"]=train.loc[train[\"is_valid\"]==1][\"name\"].apply(lambda x: ' '.join(random.sample(x.split(), len(x.split()))))\n",
    "        data_augmented[\"is_valid\"] = 1      \n",
    "        train=train.append(data_augmented)\n",
    "    \n",
    "    train_features, word_index = char2sequence(train.name,'train')\n",
    "    train_labels = np.array(train[\"is_valid\"])\n",
    "\n",
    "    test_features , word_index = char2sequence(test.name,'test')\n",
    "    test_labels = np.array(test[\"is_valid\"])\n",
    "\n",
    "\n",
    "    return train_features,train_labels,test_features,test_labels,word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadData(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_index is the vocabulary of our train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fraction=0.7\n",
    "tokenizer = Tokenizer(num_words=MAX_SEQUENCE_LENGTH, char_level=True)\n",
    "train_x,train_y,test_x,test_y,word_index = get_train_test_array(data,test_fraction,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ...,  2, 10, 22],\n",
       "       [ 0,  0,  0, ..., 10,  1,  3],\n",
       "       [ 0,  0,  0, ...,  2,  1, 14],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ..., 13,  4, 10],\n",
       "       [ 0,  0,  0, ..., 22, 11, 10],\n",
       "       [ 0,  0,  0, ..., 16,  3,  7]], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((209485, 25), (89345, 25))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape,test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding matrix is initialized as a 300 dimension embedding vector for each of the characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embedding_vector_size=300\n",
    "embeddings_index = {}\n",
    "embedding_matrix = np.random.rand(len(word_index), Embedding_vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A character level LSTM model is built with embedding layer in the start having character embeddings learnt from the train data. The general use case is to use Batch Normalization between the linear and non-linear layers in your network, because it normalizes(rescales) the input to your activation function, so that you're centered in the linear section of the activation function (such as Sigmoid).\n",
    "We should not apply dropout to output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25, 300)           17700     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 25, 100)           30100     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 25, 100)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 25, 100)           400       \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 25, 60)            31440     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 25, 60)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 25, 60)            240       \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 60)                21840     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 60)                240       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 61        \n",
      "=================================================================\n",
      "Total params: 102,021\n",
      "Trainable params: 101,581\n",
      "Non-trainable params: 440\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "auc_roc = as_keras_metric(tf.metrics.auc)\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index),Embedding_vector_size,weights=[embedding_matrix],input_length=MAX_SEQUENCE_LENGTH,trainable=True))\n",
    "model.add(Dense(100,input_shape=(train_x.shape[1],1)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Bidirectional(LSTM(30, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Bidirectional(LSTM(30)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=[auc_roc])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shuffle parameter will shuffle our entire dataset (x, y and sample_weight together) first and \n",
    "then make batches according to the batch_size argument we passed to fit. \n",
    "'class_weight' argument treats every instance of class 0 as 2 instances of class 1 means that in our loss function we assign higher value to these instances. Hence, the loss becomes a weighted average, where the weight of each sample is specified by class_weight and its corresponding class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {0: 2.,\n",
    "                1: 1.}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auc is area under the ROC curve which is better when close to 1. An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140354 samples, validate on 69131 samples\n",
      "Epoch 1/2\n",
      "140354/140354 [==============================] - 654s 5ms/step - loss: 0.1376 - auc: 0.9783 - val_loss: 0.0727 - val_auc: 0.9940\n",
      "Epoch 2/2\n",
      "140354/140354 [==============================] - 806s 6ms/step - loss: 0.0726 - auc: 0.9955 - val_loss: 0.0519 - val_auc: 0.9963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f14b90f64a8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, validation_split=0.33, nb_epoch=2, batch_size=64,class_weight=class_weight,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threshold is kept below 0.5 because we need a higher precision model with ability to precisely classify invalid names(negative class) as invalid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[59031   409]\n",
      " [  814 29091]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99     59440\n",
      "          1       0.99      0.97      0.98     29905\n",
      "\n",
      "avg / total       0.99      0.99      0.99     89345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification_and_confusion_report(test_y,y_pred,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model_1.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "#model = load_model('my_model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
